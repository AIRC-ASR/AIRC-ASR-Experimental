{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/matthew/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/matthew/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain_bidirectional\u001b[39;00m \u001b[39mimport\u001b[39;00m main\n\u001b[0;32m----> 3\u001b[0m main()\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/bidirectional_predictions/TransformerCVAE/train_bidirectional.py:336\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39m# Hack to allow tokenizing longer sequences.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m tokenizer\u001b[39m.\u001b[39mmax_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1e12\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m gpt2_model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m'\u001b[39;49m, cache_dir\u001b[39m=\u001b[39;49mcache_dir)\n\u001b[1;32m    337\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mgpt2_params:\u001b[39m\u001b[39m'\u001b[39m, num_params(gpt2_model))  \u001b[39m# gpt2: 124439808\u001b[39;00m\n\u001b[1;32m    338\u001b[0m config \u001b[39m=\u001b[39m GPT2Config()\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2362\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2361\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2362\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2364\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:948\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    947\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> 948\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPT2Model(config)\n\u001b[1;32m    949\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    951\u001b[0m     \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:678\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    681\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:678\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39;49mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    681\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:373\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m GPT2Attention(config, is_cross_attention\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, layer_idx\u001b[39m=\u001b[39mlayer_idx)\n\u001b[1;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_cross_attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m--> 373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m GPT2MLP(inner_dim, config)\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:346\u001b[0m, in \u001b[0;36mGPT2MLP.__init__\u001b[0;34m(self, intermediate_size, config)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    345\u001b[0m embed_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc \u001b[39m=\u001b[39m Conv1D(intermediate_size, embed_dim)\n\u001b[1;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj \u001b[39m=\u001b[39m Conv1D(embed_dim, intermediate_size)\n\u001b[1;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mactivation_function]\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:109\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[0;34m(self, nf, nx)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf \u001b[39m=\u001b[39m nf\n\u001b[1;32m    108\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(nx, nf)\n\u001b[0;32m--> 109\u001b[0m nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mnormal_(w, std\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m)\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(w)\n\u001b[1;32m    111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(nf))\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train_bidirectional import main\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67ac15f3b562f8a369e32143816d90bb478645f333ea27d23f6a68882860b845"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
