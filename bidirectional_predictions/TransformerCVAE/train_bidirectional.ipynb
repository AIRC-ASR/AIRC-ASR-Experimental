{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Reset cwd\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone repo\n",
    "!mkdir repo && git clone --branch \"directionTokens\" \"https://github.com/AIRC-ASR/AIRC-ASR-Experimental\" repo && mkdir \"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots\"\n",
    "\n",
    "# Download wiki plots if needed\n",
    "titlesExists = str(os.path.isfile(\"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots/titles\")).lower()\n",
    "!if ! {titlesExists}; then echo \"Downloading wikiPlots titles...\"; wget \"https://matthew-misc-bucket.s3.amazonaws.com/datasets/titles\" -O \"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots/titles\" --show-progress; fi\n",
    "\n",
    "plotsExists = str(os.path.isfile(\"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots/plots_paragraph\")).lower()\n",
    "!if ! {titlesExists}; then echo \"Downloading wikiPlots plots...\"; wget \"https://matthew-misc-bucket.s3.amazonaws.com/datasets/plots_paragraph\" -O \"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots/plots_paragraph\" --show-progress; fi\n",
    "\n",
    "!echo \"Contents of wikiPlots folder:\"\n",
    "!ls \"./repo/bidirectional_predictions/TransformerCVAE/data/wikiPlots\"\n",
    "\n",
    "# Install libraries\n",
    "!{sys.executable} -m pip install --no-cache-dir -r \"./repo/requirements.txt\"\n",
    "\n",
    "if './repo/bidirectional_predictions/TransformerCVAE/' not in sys.path:\n",
    "    sys.path.insert(0, './repo/bidirectional_predictions/TransformerCVAE/')\n",
    "\n",
    "# Change directory into CVAE\n",
    "os.chdir('./repo/bidirectional_predictions/TransformerCVAE/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/matthew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "gpt2_params: 124439808\n",
      "VAE_params: 169921536\n",
      "Done.\n",
      "Setup data...\n",
      "Batch schedule [(1, 512), (1, 1024)] 0 [512, 1024]\n",
      "Loading wikiplot dataset...\n",
      "Done.\n",
      "Train dataset size 101642\n",
      "Val dataset size 5647\n",
      "Test dataset size 5647\n",
      "Done.\n",
      "Wrapping models and optimizers...\n",
      "Done.\n",
      "Begin training iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 329/5647 [00:06<01:43, 51.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtrain_bidirectional\u001B[39;00m \u001B[39mimport\u001B[39;00m main\n\u001B[0;32m----> 3\u001B[0m main()\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/bidirectional_predictions/TransformerCVAE/train_bidirectional.py:914\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    910\u001B[0m         logger\u001B[39m.\u001B[39minfo(\u001B[39m'\u001B[39m\u001B[39m rouge : \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m'\u001B[39m, \u001B[39mstr\u001B[39m(rouge_scores_values))\n\u001B[1;32m    912\u001B[0m         VAE\u001B[39m.\u001B[39mtrain()\n\u001B[0;32m--> 914\u001B[0m test_plot(test_loader, num_iters)\n\u001B[1;32m    915\u001B[0m val_step(val_loader)\n\u001B[1;32m    916\u001B[0m generate(test_loader, num_iters)\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/bidirectional_predictions/TransformerCVAE/train_bidirectional.py:526\u001B[0m, in \u001B[0;36mmain.<locals>.test_plot\u001B[0;34m(test_loader, num_iters)\u001B[0m\n\u001B[1;32m    524\u001B[0m \u001B[39mwith\u001B[39;00m torch\u001B[39m.\u001B[39mno_grad():\n\u001B[1;32m    525\u001B[0m     \u001B[39mif\u001B[39;00m args\u001B[39m.\u001B[39mmodel_type \u001B[39m==\u001B[39m \u001B[39m'\u001B[39m\u001B[39mcvae\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[0;32m--> 526\u001B[0m         latent_mean, latent_logvar \u001B[39m=\u001B[39m VAE\u001B[39m.\u001B[39;49mencoder_prior(input_ids\u001B[39m=\u001B[39;49mx_tokens, attention_mask\u001B[39m=\u001B[39;49mx_mask)[:\u001B[39m2\u001B[39m]\n\u001B[1;32m    527\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    528\u001B[0m         latent_mean, latent_logvar \u001B[39m=\u001B[39m VAE\u001B[39m.\u001B[39mencoder(input_ids\u001B[39m=\u001B[39mx_tokens, attention_mask\u001B[39m=\u001B[39mx_mask)[:\u001B[39m2\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/bidirectional_predictions/TransformerCVAE/model.py:322\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39moutput_hidden_states:\n\u001B[1;32m    320\u001B[0m     all_hidden_states \u001B[39m=\u001B[39m all_hidden_states \u001B[39m+\u001B[39m (hidden_states\u001B[39m.\u001B[39mview(\u001B[39m*\u001B[39moutput_shape),)\n\u001B[0;32m--> 322\u001B[0m outputs \u001B[39m=\u001B[39m block(\n\u001B[1;32m    323\u001B[0m     hidden_states, layer_past\u001B[39m=\u001B[39;49mlayer_past, attention_mask\u001B[39m=\u001B[39;49mattention_mask, head_mask\u001B[39m=\u001B[39;49mhead_mask[i]\n\u001B[1;32m    324\u001B[0m )\n\u001B[1;32m    326\u001B[0m hidden_states \u001B[39m=\u001B[39m outputs[\u001B[39m0\u001B[39m]\n\u001B[1;32m    327\u001B[0m \u001B[39m# print('hidden_states', hidden_states)\u001B[39;00m\n\u001B[1;32m    328\u001B[0m \u001B[39m# if self.use_cache:\u001B[39;00m\n\u001B[1;32m    329\u001B[0m \u001B[39m#     presents = presents + (present,)\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:425\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    423\u001B[0m residual \u001B[39m=\u001B[39m hidden_states\n\u001B[1;32m    424\u001B[0m hidden_states \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mln_2(hidden_states)\n\u001B[0;32m--> 425\u001B[0m feed_forward_hidden_states \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmlp(hidden_states)\n\u001B[1;32m    426\u001B[0m \u001B[39m# residual connection\u001B[39;00m\n\u001B[1;32m    427\u001B[0m hidden_states \u001B[39m=\u001B[39m residual \u001B[39m+\u001B[39m feed_forward_hidden_states\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:352\u001B[0m, in \u001B[0;36mGPT2MLP.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, hidden_states: Optional[Tuple[torch\u001B[39m.\u001B[39mFloatTensor]]) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m torch\u001B[39m.\u001B[39mFloatTensor:\n\u001B[0;32m--> 352\u001B[0m     hidden_states \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mc_fc(hidden_states)\n\u001B[1;32m    353\u001B[0m     hidden_states \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mact(hidden_states)\n\u001B[1;32m    354\u001B[0m     hidden_states \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mc_proj(hidden_states)\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49m\u001B[39minput\u001B[39;49m, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1195\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/pgrm/AIRC-ASR-Experimental/venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:115\u001B[0m, in \u001B[0;36mConv1D.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, x):\n\u001B[1;32m    114\u001B[0m     size_out \u001B[39m=\u001B[39m x\u001B[39m.\u001B[39msize()[:\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m] \u001B[39m+\u001B[39m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mnf,)\n\u001B[0;32m--> 115\u001B[0m     x \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39;49maddmm(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mbias, x\u001B[39m.\u001B[39;49mview(\u001B[39m-\u001B[39;49m\u001B[39m1\u001B[39;49m, x\u001B[39m.\u001B[39;49msize(\u001B[39m-\u001B[39;49m\u001B[39m1\u001B[39;49m)), \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mweight)\n\u001B[1;32m    116\u001B[0m     x \u001B[39m=\u001B[39m x\u001B[39m.\u001B[39mview(size_out)\n\u001B[1;32m    117\u001B[0m     \u001B[39mreturn\u001B[39;00m x\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from train_bidirectional import main\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67ac15f3b562f8a369e32143816d90bb478645f333ea27d23f6a68882860b845"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
